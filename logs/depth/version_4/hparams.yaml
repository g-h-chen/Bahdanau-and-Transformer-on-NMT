data:
  train_length: 1000000
  val_length: 39323
  train_batch_size: 80
  val_batch_size: 80
trainer:
  gpus: 3
  precision: 16
  max_epochs: 11
  num_nodes: 1
  enable_progress_bar: false
logger:
  save_dir: logs
  name: depth
  version: 4
seq2seqtransformer:
  num_encoder_layers: 15
  num_decoder_layers: 15
  maxlen: 256
  emb_size: 768
  n_head: 8
  learning_rate: 0.003
  dropout: 0.1
  weight_decay: 0.0
  src_tokenizer_name: bert-base-uncased
  tgt_tokenizer_name: bert-base-chinese
optimizer:
  general:
    weight_decay: 0.0
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-09
scheduler:
  warmup_steps: 6000
  verbose: false
  monitor: val/loss
  frequency: 1
  interval: step
